Fitness functions based on test cases are very common in Genetic Programming
This process can be assimilated to a learning task, with the inference of models from a limited number of samples
This paper is an investigation on two methods to improve generalization in GP-based learning the selection of the best-of-run individuals using a three data sets methodology, and the application of parsimony pressure in order to reduce the complexity of the solutions
Results using GP in a binary classification setup show that while the accuracy on the test sets is preserved, with less variances compared to baseline results, the mean tree size obtained with the tested methods is significantly reduced
GP is particularly suited for problems that can be assimilated to learning tasks, with the minimization of the error between the obtained and desired outputs for a limited number of test cases the training data, using a ML terminology
Indeed, the classical GP examples of symbolic regression, boolean multiplexer and artificial ant are only simple instances of well-known learning problems i e respectively regression, binary classification and reinforcement learning
In the early years of GP, these problems were tackled using a single data set, reporting results on the same data set that was used to evaluate the fitnesses during the evolution
This was justifiable by the fact that these are toy problems used only to illustrate the potential of GP
In the ML community, it is recognized that such methodology is flawed, given that the learning algorithm can overfit the data used during the training and perform poorly on unseen data of the same application domain 
Hence, it is important to report results on a set of data that was not used during the learning stage
This is what we call in this paper a two data sets methodology , with a training set used by the learning algorithm and a test set used to report the performance of the algorithm on unseen data, which is a good indicator of the algorithm's generalization or robustness capability
Even though this methodology has been widely accepted and applied in the ML and PR communities for a long time, the EC community still lags behind by publishing papers that are reporting results on data sets that were used during the evolution training phase
This methodological problem has already been spotted see and should be less and less common in the future
The two data sets methodology prevents reporting flawed results of learning algorithms that overfit the training set
But this does not prevent by itself overfitting the training set
A common approach is to add a third data set the validation set which helps the learning algorithm to measure its generalization capability
This validation set is useful to interrupt the learning algorithm when overfitting occurs and   or select a configuration of the learning machine that maximizes the generalization performances
This third data set is commonly used to train classifiers such as back-propagation neural networks and can be easily applied to EC-based learning
But this approach has an important drawback: it removes a significant amount of data from the training set, which can be harmful to the learning process
Indeed, the richer the training set, the more representative it can be of the real data distribution, and the more the learning algorithm can be expected to converge toward robust solutions
In the light of these considerations, an objective of this paper is to investigate the effect of a validation set to select the best-of-run individuals for a GP-based learning application
Another concern of the ML and PR communities is to develop learning algorithms that generate simple solutions
An argument behind this is the Occam's Razor principle, which states that between solutions of comparable quality, the simplest solutions must be preferred
Another argument is the minimum description length principle , which states that the ``best'' model is the one that minimizes the amount of information needed to encode the model and the data given the model
Preference for simpler solutions and overfitting avoidance are closely related: it is more likely that a complex solution incorporates specific information from the training set, thus overfitting the training set, compared to a simpler solution
But, as mentioned in , this argumentation should be taken with care as too much emphasis on minimizing complexity can prevent the discovery of more complex yet more accurate solutions
There is a strong link between the minimization of complexity in GP-based learning and the control of code bloat , that is an exaggerated growth of program size in the course of GP runs
Even though complexity and code bloat are not exactly the same phenomenon, as some kind of bloat is generated by neutral pieces of code that have no effect on the actual complexity of the solutions, most of the mechanisms proposed to control it can also be used to minimize the complexity of solutions obtained by GP-based learning
This paper is a study of GP viewed as a learning algorithm
More specifically, we investigate two techniques to increase the generalization performance and decrease the complexity of the models use of a validation set to select best-of-run individuals that generalize well, and use of lexicographic parsimony pressure to reduce the complexity of the generated models
These techniques are tested using a GP encoding for binary classification problems, with vectors taken from the learning sets as terminals, and mathematical operations to manipulate these vectors as branches
This approach is tested on six different data sets from the UCI ML repository 
Even if the proposed techniques are tested in a specific context, we argue that they can be extended to the frequent situations where GP is used as a learning algorithm
The problem of joint universal source coding and modeling, addressed by Rissanen in the context of lossless codes, is generalized to fixed-rate lossy coding of continuous-alphabet memoryless sources
We show that, for bounded distortion measures, any compactly parametrized family of real vector sources with absolutely continuous marginals satisfying appropriate smoothness and Vapnik Chervonenkis learnability conditions admits a joint scheme for universal lossy block coding and parameter estimation, and give nonasymptotic estimates of convergence rates for distortion redundancies and variational distances between the active source and the estimated source
We also present explicit examples of parametric sources admitting such joint universal compression and modeling schemes
In universal data compression, a single code achieves asymptotically optimal performance on all sources within a given family
Intuition suggests that a good universal coder should acquire an accurate model of the source statistics from a sufficiently long data sequence and incorporate this knowledge in its operation
For loss-less codes, this intuition has been made rigorous by Rissanen 
Under his scheme, the data are encoded in a two-stage set-up, in which the binary representation of each source block consists of two parts: a suitably quantized maximum-likelihood estimate of the source parameters, and loss-less encoding of the data matched to the acquired model; the redundancy of the resulting code converges to zero as SYMBOL , where SYMBOL is the block length
In this paper, we extend Rissanen's idea to lossy block coding vector quantization of iid sources with values in SYMBOL for some finite SYMBOL
Specifically, let SYMBOL be a source with the marginal distribution of SYMBOL belonging to some indexed class SYMBOL of absolutely continuous distributions on SYMBOL , where SYMBOL is a bounded subset of SYMBOL for some SYMBOL
For bounded distortion measures, our main result, Theorem , states that if the class SYMBOL satisfies certain smoothness and learnability conditions, then there exists a sequence of finite-memory lossy block codes that achieves asymptotically optimal compression of each source in the class and permits asymptotically exact identification of the active source with respect to the variational distance , defined as SYMBOL , where the supremum is over all Borel subsets of SYMBOL
The overhead rate and the distortion redundancy of the scheme converge to zero as SYMBOL and SYMBOL , respectively, where SYMBOL is the block length, while the active source can be identified up to a variational ball of radius SYMBOL eventually almost surely
We also describe an extension of our scheme to unbounded distortion measures satisfying a certain moment condition, and present two examples of parametric families satisfying the regularity conditions of Theorem 
While most existing schemes for universal lossy coding rely on implicit identification of the active source e.g. , through topological covering arguments , Glivenko Cantelli uniform laws of large numbers , or nearest-neighbor code clustering , our code builds an explicit model of the mechanism responsible for generating the data and then selects an appropriate code for the data on the basis of the model
This ability to simultaneously model and compress the data may prove useful in such applications as media forensics , where the parameter SYMBOL could represent evidence of tampering, and the aim is to compress the data in such a way that the evidence can be later extracted with high fidelity from the compressed version
Another key feature of our approach is the use of Vapnik Chervonenkis theory in order to connect universal encodability of a class of sources to the combinatorial ``richness" of a certain collection of decision regions associated with the sources
In a way, Vapnik Chervonenkis estimates can be thought of as an imperfect analogue of the combinatorial method of types for finite alphabets 
This paper addresses the problem of distributed learning under communication constraints, motivated by distributed signal processing in wireless sensor networks and data mining with distributed databases
After formalizing a general model for distributed learning, an algorithm for collaboratively training regularized kernel least-squares regression estimators is derived
Noting that the algorithm can be viewed as an application of successive orthogonal projection algorithms, its convergence properties are investigated and the statistical behavior of the estimator is discussed in a simplified theoretical setting
In this paper, we address the problem of distributed learning under communication constraints , motivated primarily by distributed signal processing in wireless sensor networks WSNs and data mining with distributed databases
WSNs are a fortiori designed to make inferences from the environments they are sensing; however they are typically characterized by constraints on energy and bandwidth, which limit the sensors' ability to share data with each other or with a centralized fusion center
In data mining with distributed databases, multiple agents e g , corporations have access to possibly overlapping databases, and wish to collaborate to make optimal inferences; privacy or security concerns, however, may preclude them from fully sharing information
Nonparametric methods studied within machine learning have demonstrated widespread empirical success in many centralized i e , communication unconstrained signal processing applications
Thus, in both the aforementioned applications, a natural question arises: can the power of machine learning methods be tapped for nonparametric inference in distributed learning under communication constraints
In this paper, we address this question by formalizing a general model for distributed learning, and then deriving a distributed algorithm for collaborative training in regularized kernel least-squares regression
The algorithm can be viewed as an instantiation of successive orthogonal projection algorithms, and thus, insight into the statistical behavior of these algorithms can be gleaned from standard analyses in mathematical programming
Given a finite set of words SYMBOL independently drawn according to a fixed unknown distribution law SYMBOL called a stochastic language , an usual goal in Grammatical Inference is to infer an estimate of SYMBOL in some class of probabilistic models, such as Probabilistic Automata PA
Here, we study the class SYMBOL of rational stochastic languages , which consists in stochastic languages that can be generated by Multiplicity Automata MA and which strictly includes the class of stochastic languages generated by PA
Rational stochastic languages have minimal normal representation which may be very concise, and whose parameters can be efficiently estimated from stochastic samples
We design an efficient inference algorithm DEES which aims at building a minimal normal representation of the target
Despite the fact that no recursively enumerable class of MA computes exactly SYMBOL , we show that DEES strongly identifies SYMBOL in the limit
We study the intermediary MA output by DEES and show that they compute rational series which converge absolutely to one and which can be used to provide stochastic languages which closely estimate the target
In probabilistic grammatical inference, it is supposed that data arise in the form of a finite set of words SYMBOL , built on a predefinite alphabet SYMBOL , and independently drawn according to a fixed unknown distribution law on SYMBOL called a stochastic language
Then, an usual goal is to try to infer an estimate of this distribution law in some class of probabilistic models, such as Probabilistic Automata PA , which have the same expressivity as Hidden Markov Models HMM PA are identifiable in the limit 
However, to our knowledge, there exists no efficient inference algorithm able to deal with the whole class of stochastic languages that can be generated from PA
Most of the previous works use restricted subclasses of PA such as Probabilistic Deterministic Automata PDA 
In the other hand, Probabilistic Automata are particular cases of Multiplicity Automata , and stochastic languages which can be generated by multiplicity automata are special cases of rational languages that we call rational stochastic languages
MA have been used in grammatical inference in a variant of the exact learning model of Angluin but not in probabilistic grammatical inference
Let us design by SYMBOL , the class of rational stochastic languages over the semiring SYMBOL
When SYMBOL or SYMBOL , SYMBOL is exactly the class of stochastic languages generated by PA with parameters in SYMBOL
But, when SYMBOL or SYMBOL , we obtain strictly greater classes which provide several advantages and at least one drawback: elements of SYMBOL may have significantly smaller representation in SYMBOL which is clearly an advantage from a learning perspective; elements of SYMBOL have a minimal normal representation while such normal representations do not exist for PA; parameters of these minimal representations are directly related to probabilities of some natural events of the form SYMBOL , which can be efficiently estimated from stochastic samples; lastly, when SYMBOL is a field, rational series over SYMBOL form a vector space and efficient linear algebra techniques can be used to deal with rational stochastic languages
However, the class SYMBOL presents a serious drawback : there exists no recursively enumerable subset of MA which exactly generates it 
Moreover, this class of representations is unstable: arbitrarily close to an MA which generates a stochastic language, we may find MA whose associated rational series SYMBOL takes negative values and is not absolutely convergent: the global weight SYMBOL may be unbounded or not absolutely defined
However, we show that SYMBOL is strongly identifiable in the limit: we design an algorithm DEES such that, for any target SYMBOL and given access to an infinite sample SYMBOL drawn according to SYMBOL , will converge in a finite but unbounded number of steps to a minimal normal representation of SYMBOL
Moreover, DEES is efficient: it runs within polynomial time in the size of the input and it computes a minimal number of parameters with classical statistical rates of convergence
However, before converging to the target, DEES output MA which are close to the target but which do not compute stochastic languages
The question is: what kind of guarantees do we have on these intermediary hypotheses and how can we use them for a probabilistic inference purpose
We show that, since the algorithm aims at building a minimal normal representation of the target, the intermediary hypotheses SYMBOL output by DEES have a nice property: they absolutely converge to 1, i e SYMBOL and SYMBOL
As a consequence, SYMBOL is defined without ambiguity for any SYMBOL , and it can be shown that SYMBOL tends to 0 as the learning proceeds
Given any such series SYMBOL , we can efficiently compute a stochastic language SYMBOL , which is not rational, but has the property that SYMBOL for any word SYMBOL such that SYMBOL
Our conclusion is that, despite the fact that no recursively enumerable class of MA represents the class of rational stochastic languages, MA can be used efficiently to infer such stochastic languages
Classical notions on stochastic languages, rational series, and multiplicity automata are recalled in Section
We study an example which shows that the representation of rational stochastic languages by MA with real parameters may be very concise
We introduce our inference algorithm DEES in Section and we show that SYMBOL is strongly indentifiable in the limit
We study the properties of the MA output by DEES in Section and we show that they define absolutely convergent rational series which can be used to compute stochastic languages which are estimates of the target
Consider the problem of joint parameter estimation and prediction in a Markov random field: ie , the model parameters are estimated on the basis of an initial set of data, and then the fitted model is used to perform prediction e g , smoothing, denoising, interpolation on a new noisy observation
Working under the restriction of limited computation, we analyze a joint method in which the same convex variational relaxation is used to construct an M-estimator for fitting parameters, and to perform approximate marginalization for the prediction step
The key result of this paper is that in the computation-limited setting, using an inconsistent parameter estimator i e , an estimator that returns the ``wrong'' model even in the infinite data limit can be provably beneficial, since the resulting errors can partially compensate for errors made by using an approximate prediction technique
En route to this result, we analyze the asymptotic properties of M-estimators based on convex variational relaxations, and establish a Lipschitz stability property that holds for a broad class of variational methods
We show that joint estimation   prediction based on the reweighted sum-product algorithm substantially outperforms a commonly used heuristic based on ordinary sum-product
Graphical models such as Markov random fields MRFs are widely used in many application domains, including spatial statistics, statistical signal processing, and communication theory
A fundamental limitation to their practical use is the infeasibility of computing various statistical quantities e g , marginals, data likelihoods etc ; such quantities are of interest both Bayesian and frequentist settings
Sampling-based methods, especially those of the Markov chain Monte Carlo MCMC variety , represent one approach to obtaining stochastic approximations to marginals and likelihoods
A disadvantage of sampling methods is their relatively high computational cost
For instance, in applications with severe limits on delay and computational overhead e g , error-control coding, real-time tracking, video compression , MCMC methods are likely to be overly slow
It is thus of considerable interest for various application domains to consider less computationally intensive methods for generating approximations to marginals, log likelihoods, and other relevant statistical quantities
Variational methods are one class of techniques that can be used to generate deterministic approximations in Markov random fields MRFs 
At the foundation of these methods is the fact that for a broad class of MRFs, the computation of the log likelihood and marginal probabilities can be reformulated as a convex optimization problem see for an overview 
Although this optimization problem is intractable to solve exactly for general MRFs, it suggests a principled route to obtaining approximations -namely, by relaxing the original optimization problem, and taking the optimal solutions to the relaxed problem as approximations to the exact values
In many cases, optimization of the relaxed problem can be carried out by ``message-passing'' algorithms, in which neighboring nodes in the Markov random field convey statistical information e g , likelihoods by passing functions or vectors referred to as messages 
Estimating the parameters of a Markov random field from data poses another significant challenge
A direct approach -for instance, via regularized maximum likelihood estimation -entails evaluating the cumulant generating or log partition function, which is computationally intractable for general Markov random fields
One viable option is the pseudolikelihood method , which can be shown to produce consistent parameter estimates under suitable assumptions, though with an associated loss of statistical efficiency
Other researchers have studied algorithms for ML estimation based on stochastic approximation , which again are consistent under appropriate assumptions, but can be slow to converge
The goal of the present paper is to provide a systematic and comprehensive study of rational stochastic languages over a semiring SYMBOL
A rational stochastic language is a probability distribution over a free monoid SYMBOL which is rational over SYMBOL , that is which can be generated by a multiplicity automata with parameters in SYMBOL
We study the relations between the classes of rational stochastic languages SYMBOL
We define the notion of residual of a stochastic language and we use it to investigate properties of several subclasses of rational stochastic languages
Lastly, we study the representation of rational stochastic languages by means of multiplicity automata
In probabilistic grammatical inference, data often arise in the form of a finite sequence of words SYMBOL over some predefined alphabet SYMBOL
These words are assumed to be independently drawn according to a fixed but unknown probability distribution over SYMBOL
Probability distributions over free monoids SYMBOL are called stochastic languages
A usual goal in grammatical inference is to try to infer an approximation of this distribution in some class of probabilistic models, such as probabilistic automata
A probabilistic automaton PA is composed of a structure , which is a finite automaton NFA , and parameters associated with states and transitions, which represent the probability for a state to be initial, terminal or the probability for a transition to be chosen
It can easily be shown that probabilistic automata have the same expressivity as Hidden Markov Models HMM , which are heavily used in statistical inference 
Given the structure SYMBOL of a probabilistic automaton and a sequence of words SYMBOL , computing parameters for SYMBOL which maximize the likelihood of SYMBOL is NP-hard 
In practical cases however, algorithms based on the E M Expectation-Maximization method can be used to compute approximate values
On the other hand, inferring a probabilistic automaton structure and parameters from a sequence of words is a widely open field of research
Most results obtained so far only deal with restricted subclasses of PA, such as Probabilistic Deterministic Automata PDA , i e probabilistic automata whose structure is deterministic DFA or Probabilistic Residual Automata PRA , i e probabilistic automata whose structure is a residual finite state automaton RFSA 
In other respects, it can be noticed that stochastic languages are particular cases of formal power series and that probabilistic automata are also particular cases of multiplicity automata , notions which have been extensively studied in the field of formal language theory 
Therefore, stochastic languages which can be generated by multiplicity automata are special cases of rational languages
We call them rational stochastic languages
The goal of the present paper is to provide a systematic and comprehensive study of rational stochastic languages so as to bring out properties that could be useful for a grammatical inference purpose
Indeed, considering the objects to infer as special cases of rational languages makes it possible to use the powerful theoretical tools that have been developed in that field and hence, give answers to many questions that naturally arise when working with them: is it possible to decide within polynomial time whether two probabilistic automata generate the same stochastic language does allowing negative coefficients in probabilistic automata extend the class of generated stochastic languages can a rational stochastic language which takes all its values in SYMBOL always be generated by a multiplicity automata with coefficients in SYMBOL and so forth
Also, studying rational stochastic languages for themselves, considered as objects of language theory, helps to bring out notions and properties which are important in a grammatical inference pespective: for example, we show that the notion of residual language or derivative , so important for grammatical inference , has a natural counterpart for stochastic languages , which can be used to express many properties of classes of stochastic languages
Formal power series take their values in a semi-ring SYMBOL : let us denote by SYMBOL the set of all formal power series
Here, we only consider semi-rings SYMBOL , SYMBOL , SYMBOL and SYMBOL
For any such semi-ring SYMBOL , we define the set SYMBOL of rational stochastic languages as the set of stochastic languages over SYMBOL which are rational languages over SYMBOL
For any two distinct semi-rings SYMBOL and SYMBOL , the corresponding sets of rational stochastic languages are distinct
We show that SYMBOL is a Fatou extension of SYMBOL for stochastic languages, which means that any rational stochastic language over SYMBOL which takes its values in SYMBOL is also rational over SYMBOL
However, SYMBOL is not a Fatou extension of SYMBOL for stochastic languages: there exists a rational stochastic language over SYMBOL which takes its values in SYMBOL and which is not rational over SYMBOL
For any stochastic language SYMBOL over SYMBOL and any word SYMBOL such that SYMBOL , let us define the residual language SYMBOL of SYMBOL with respect to SYMBOL by SYMBOL : residual languages clearly are stochastic languages
We show that the residual languages of a rational stochastic language SYMBOL over SYMBOL are also rational over SYMBOL
The residual submodule SYMBOL of SYMBOL spanned by the residual languages of any stochastic language SYMBOL may be used to express the rationality of SYMBOL : SYMBOL is rational iff SYMBOL is included in a finitely generated submodule of SYMBOL
But when SYMBOL is positive, i e SYMBOL or SYMBOL , it may happen that SYMBOL itself is not finitely generated
We study the properties of two subclasses of SYMBOL : the set SYMBOL composed of rational stochastic languages over SYMBOL whose residual module is finitely generated and the set SYMBOL composed of rational stochastic languages over SYMBOL which have finitely many residual languages
We show that for any of these two classes, SYMBOL is a Fatou extension of SYMBOL : any stochastic language of SYMBOL
of SYMBOL which takes its values in SYMBOL is an element of SYMBOL
We also show that for any element SYMBOL of SYMBOL , there exists a unique minimal subset of residual languages of SYMBOL which generates SYMBOL
Then, we study the representation of rational stochastic languages by means of multiplicity automata
We first show that the set of multiplicity automata with parameters in SYMBOL which generate stochastic languages is not recursive
Moreover, it contains no recursively enumerable subset capable to generate the whole set of rational stochastic languages over SYMBOL
A stochastic language SYMBOL is a formal series which has two properties: i SYMBOL for any word SYMBOL , ii SYMBOL
We show that the undecidability comes from the first requirement, since the second one can be decided within polynomial time
We show that the set of stochastic languages which can be generated by probabilistic automata with parameters in SYMBOL exactly coincides with SYMBOL
A probabilistic automaton SYMBOL is called a Probabilistic Residual Automaton PRA if the stochastic languages associated with its states are residual languages of the stochastic languages SYMBOL generated by SYMBOL
We show that the set of stochastic languages that can be generated by probabilistic residual automata with parameters in SYMBOL exactly coincides with SYMBOL resp
We do not know whether the class of PRA is decidable
However, we describe two decidable subclasses of PRA capable of generating SYMBOL when SYMBOL or SYMBOL : the class of SYMBOL -reduced PRA and the class of prefixial PRA
The first one provides minimal representation in the class of PRA but we show that the membership problem is PSPACE-complete
The second one produces more cumbersome representation but the membership problem is polynomial
Finally, we show that the set of stochastic languages that can be generated by probabilistic deterministic automata with parameters in SYMBOL exactly coincides with SYMBOL , which is also equal to SYMBOL , which is also equal to SYMBOL
We recall some properties on rational series, stochastic languages and multiplicity automata in Section 
We define and study rational stochastic languages in Section
The relations between the classes of rational stochastic languages are studied in Subsection 
Properties of the residual languages of rational stochastic languages are studied in Subsection 
A characterisation of rational stochastic languages in terms of stable subsemimodule is given in Subsection 
Classes SYMBOL and SYMBOL are defined and studied in Subsection
The representation of rational stochastic languages by means of multiplicity automata is given in Section
We address the problem of autonomously learning controllers for vision-capable mobile robots
We extend McCallum's Nearest-Sequence Memory algorithm to allow for general metrics over state-action trajectories
We demonstrate the feasibility of our approach by successfully running our algorithm on a real mobile robot
The algorithm is novel and unique in that it a explores the environment and learns directly on a mobile robot without using a hand-made computer model as an intermediate step, b does not require manual discretization of the sensor input space, c works in piecewise continuous perceptual spaces, and d copes with partial observability
Together this allows learning from much less experience compared to previous methods
The realization of fully autonomous robots will require algorithms that can learn from direct experience obtained from visual input
Vision systems provide a rich source of information, but, the piecewise-continuous PWC structure of the perceptual space e g video images implied by typical mobile robot environments is not compatible with most current, on-line reinforcement learning approaches
These environments are characterized by regions of smooth continuity separated by discontinuities that represent the boundaries of physical objects or the sudden appearance or disappearance of objects in the visual field
There are two broad approaches that are used to adapt existing algorithms to real world environments: 1 discretizing the state space with fixed or adaptive grids, and 2 using a function approximator such as a neural-network , radial basis functions RBFs , CMAC , or instance-based memory 
Fixed discrete grids introduce artificial discontinuities, while adaptive ones scale exponentially with state space dimensionality
Neural networks implement relatively smooth global functions that are not capable of approximating discontinuities, and RBFs and CMACs, like fixed grid methods, require knowledge of the appropriate local scale
Instance-based methods use a neighborhood of explicitly stored experiences to generalize to new experiences
These methods are more suitable for our purposes because they implement local models that in principle can approximate PWC functions, but typically fall short because, by using a fixed neighborhood radius, they assume a uniform sampling density on the state space
A fixed radius prevents the approximator from clearly identifying discontinuities because points on both sides of the discontinuity can be averaged together, thereby blurring its location
If instead we use a fixed number SYMBOL of neighbors in effect using a variable radius the approximator has arbitrary resolution near important state space boundaries where it is most needed to accurately model the local dynamics
To use such an approach, an appropriate metric is needed to determine which stored instances provide the most relevant information for deciding what to do in a given situation 
Apart from the PWC structure of the perceptual space, a robot learning algorithm must also cope with the fact that instantaneous sensory readings alone rarely provide sufficient information for the robot to determine where it is localization problem and what action it is best to take
Some form of short-term memory is needed to integrate successive inputs and identify the underlying environment states that are otherwise only partially observable
In this paper, we present an algorithm called Piecewise Continuous Nearest Sequence Memory PC-NSM that extends McCallum's instance-based algorithm for discrete, partially observable state spaces, Nearest Sequence Memory NSM; , to the more general PWC case
Like NSM, PC-NSM stores all the data it collects from the environment, but uses a continuous metric on the history that allows it to be used in real robot environments without prior discretization of the perceptual space
An important priority in this work is minimizing the amount of a priori knowledge about the structure of the environment that is available to the learner
Typically, artificial learning is conducted in simulation, and then the resulting policy is transfered to the real robot
Building an accurate model of a real environment is human-resource intensive and only really achievable when simple sensors are used unlike full-scale vision , while overly simplified models make policy transfer difficult 
For this reason, we stipulate that the robot must learn directly from the real world
Furthermore, since gathering data in the real world is costly, the algorithm should be capable of efficient autonomous exploration in the robot perceptual state space without knowing the amount of exploration required in different parts of the state space as is normally the case in even the most advanced approaches to exploration in discrete , and even in metric state spaces 
The next section introduces PC-NSM, section presents our experiments in robot navigation, and section discusses our results and future directions for our research
A method of topological grammars is proposed for multidimensional data approximation
For data with complex topology we define a principal cubic complex of low dimension and given complexity that gives the best approximation for the dataset
This complex is a generalization of linear and non-linear principal manifolds and includes them as particular cases
The problem of optimal principal complex construction is transformed into a series of minimization problems for quadratic functionals
These quadratic functionals have a physically transparent interpretation in terms of elastic energy
For the energy computation, the whole complex is represented as a system of nodes and springs
Topologically, the principal complex is a product of one-dimensional continuums represented by graphs , and the grammars describe how these continuums transform during the process of optimal complex construction
This factorization of the whole process onto one-dimensional transformations using minimization of quadratic energy functionals allow us to construct efficient algorithms
In this paper, we discuss a classical problem: how to approximate a finite set SYMBOL in SYMBOL for relatively large SYMBOL by a finite subset of a regular low-dimensional object in SYMBOL
In application, this finite set is a dataset, and this problem arises in many areas: from data visualization to fluid dynamics
The first hypothesis we have to check is: whether the dataset SYMBOL is situated near a low dimensional affine manifold plane in SYMBOL
If we look for a point, straight line, plane, that minimizes the average squared distance to the datapoints, we immediately come to the Principal Component Analysis PCA 
PCA is one of the most seminal inventions in data analysis now it is textbook material
Nonlinear generalization of PCA is a great challenge, and many attempts have been made to answer it
Two of them are especially important for our consideration: Kohonen's Self-Organizing Maps SOM and principal manifolds
With the SOM algorithm we take a finite metric space SYMBOL with metric SYMBOL and try to map it into SYMBOL with a the best preservation of initial structure in the image of SYMBOL and b the best approximation of the dataset SYMBOL
The SOM algorithm has several setup variables to regulate the compromise between these goals
We start from some initial approximation of the map, SYMBOL
On each SYMBOL -th step of the algorithm we have a datapoint SYMBOL and a current approximation SYMBOL
For these SYMBOL and SYMBOL we define an owner of SYMBOL in SYMBOL : SYMBOL
The next approximation, SYMBOL , is SYMBOL Here SYMBOL is a step size, SYMBOL is a monotonically decreasing cutting function
There are many ways to combine steps in the whole algorithm
The idea of SOM is very flexible and seminal, has plenty of applications and generalizations, but, strictly speaking, we don't know what we are looking for: we have the algorithm, but no independent definition: SOM is a result of the algorithm work
The attempts to define SOM as solution of a minimization problem for some energy functional were not very successful 
For a known probability distribution, principal manifolds were introduced as lines or surfaces passing through ``the middle'' of the data distribution 
This intuitive vision was transformed into the mathematical notion of self-consistency : every point SYMBOL of the principal manifold SYMBOL is a conditional expectation of all points SYMBOL that are projected into SYMBOL
Neither manifold, nor projection should be linear: just a differentiable projection SYMBOL of the data space usually it is SYMBOL or a domain in SYMBOL onto the manifold SYMBOL with the self-consistency requirement for conditional expectations: SYMBOL For a finite dataset SYMBOL , only one or zero datapoints are typically projected into a point of the principal manifold
In order to avoid overfitting, we have to introduce smoothers that become an essential part of the principal manifold construction algorithms
SOMs give the most popular approximations for principal manifolds: we can take for SYMBOL a fragment of a regular SYMBOL -dimensional grid and consider the resulting SOM as the approximation to the SYMBOL -dimensional principal manifold see, for example, 
Several original algorithms for construction of principal curves and surfaces for finite datasets were developed during last decade, as well as many applications of this idea
In 1996, in a discussion about SOM at the 5th Russian National Seminar in Neuroinformatics, a method of multidimensional data approximation based on elastic energy minimization was proposed see and the bibliography there 
This method is based on the analogy between the principal manifold and the elastic membrane and plate 
Following the metaphor of elasticity, we introduce two quadratic smoothness penalty terms
This allows one to apply standard minimization of quadratic functionals i e , solving a system of linear algebraic equations with a sparse matrix 
We address the problem of reinforcement learning in which observations may exhibit an arbitrary form of stochastic dependence on past observations and actions
The task for an agent is to attain the best possible asymptotic reward where the true generating environment is unknown but belongs to a known countable family of environments
We find some sufficient conditions on the class of environments under which an agent exists which attains the best asymptotic reward for any environment in the class
We analyze how tight these conditions are and how they relate to different probabilistic assumptions known in reinforcement learning and related fields, such as Markov Decision Processes and mixing conditions
Many real-world ``learning'' problems like learning to drive a car or playing a game can be modelled as an agent SYMBOL that interacts with an environment SYMBOL and is occasionally rewarded for its behavior
We are interested in agents which perform well in the sense of having high long-term reward, also called the value SYMBOL of agent SYMBOL in environment SYMBOL
If SYMBOL is known, it is a pure non-learning computational problem to determine the optimal agent SYMBOL
It is far less clear what an ``optimal'' agent means, if SYMBOL is unknown
A reasonable objective is to have a single policy SYMBOL with high value simultaneously in many environments
We will formalize and call this criterion self-optimizing later
Reinforcement learning, sequential decision theory, adaptive control theory, and active expert advice, are theories dealing with this problem
They overlap but have different core focus: Reinforcement learning algorithms are developed to learn SYMBOL or directly its value
Temporal difference learning is computationally very efficient, but has slow asymptotic guarantees only in effectively small observable MDPs
Others have faster guarantee in finite state MDPs 
There are algorithms which are optimal for any finite connected POMDP, and this is apparently the largest class of environments considered
In sequential decision theory, a Bayes-optimal agent SYMBOL that maximizes SYMBOL is considered, where SYMBOL is a mixture of environments SYMBOL and SYMBOL is a class of environments that contains the true environment SYMBOL 
Policy SYMBOL is self-optimizing in an arbitrary class SYMBOL , provided SYMBOL allows for self-optimizingness 
Adaptive control theory considers very simple from an AI perspective or special systems e g     linear with quadratic loss function , which sometimes allow computationally and data efficient solutions
Action with expert advice constructs an agent called master that performs nearly as well as the best agent best expert in hindsight from some class of experts, in any environment SYMBOL
The important special case of passive sequence prediction in arbitrary unknown environments, where the actions=predictions do not affect the environment is comparably easy 
The difficulty in active learning problems can be identified at least, for countable classes with traps in the environments
Initially the agent does not know SYMBOL , so has asymptotically to be forgiven in taking initial ``wrong'' actions
A well-studied such class are ergodic MDPs which guarantee that, from any action history, every state can be re visited
The aim of this paper is to characterize as general as possible classes SYMBOL in which self-optimizing behaviour is possible, more general than POMDPs
To do this we need to characterize classes of environments that forgive
For instance, exact state recovery is unnecessarily strong; it is sufficient being able to recover high rewards, from whatever states
Further, in many real world problems there is no information available about the ``states'' of the environment e g     in POMDPs or the environment may exhibit long history dependencies
Rather than trying to model an environment e g by MDP we try to identify the conditions sufficient for learning
Towards this aim, we propose to consider only environments in which, after any arbitrary finite sequence of actions, the best value is still achievable
The performance criterion here is asymptotic average reward
Thus we consider such environments for which there exists a policy whose asymptotic average reward exists and upper-bounds asymptotic average reward of any other policy
Moreover, the same property should hold after any finite sequence of actions has been taken no traps 
Yet this property in itself is not sufficient for identifying optimal behavior
We require further that, from any sequence of SYMBOL actions, it is possible to return to the optimal level of reward in SYMBOL steps The above conditions will be formulated in a probabilistic form Environments which possess this property are called strongly value-stable
We show that for any countable class of value-stable environments there exists a policy which achieves best possible value in any of the environments from the class i e is self-optimizing for this class 
We also show that strong value-stability is in a certain sense necessary
We also consider examples of environments which possess strong value-stability
In particular, any ergodic MDP can be easily shown to have this property
A mixing-type condition which implies value-stability is also demonstrated
Finally, we provide a construction allowing to build examples of value-stable environments which are not isomorphic to a finite POMDP, thus demonstrating that the class of value-stable environments is quite general
It is important in our argument that the class of environments for which we seek a self-optimizing policy is countable, although the class of all value-stable environments is uncountable
To find a set of conditions necessary and sufficient for learning which do not rely on countability of the class is yet an open problem
However, from a computational perspective countable classes are sufficiently large e g     the class of all computable probability measures is countable
The paper is organized as follows
Section introduces necessary notation of the agent framework
In Section we define and explain the notion of value-stability, which is central in the paper
Section presents the theorem about self-optimizing policies for classes of value-stable environments, and illustrates the applicability of the theorem by providing examples of strongly value-stable environments
In Section we discuss necessity of the conditions of the main theorem
Section provides some discussion of the results and an outlook to future research
The formal proof of the main theorem is given in the appendix, while Section contains only intuitive explanations
While in general trading off exploration and exploitation in reinforcement learning is hard, under some formulations relatively simple solutions exist
Optimal decision thresholds for the multi-armed bandit problem, one for the infinite horizon discounted reward case and one for the finite horizon undiscounted reward case are derived, which make the link between the reward horizon, uncertainty and the need for exploration explicit
From this result follow two practical approximate algorithms, which are illustrated experimentally
In reinforcement learning, the dilemma between selecting actions to maximise the expected return according to the current world model and to improve the world model such as to potentially be able to achieve a higher expected return is referred to as the exploration-exploitation trade-off
This has been the subject of much interest before, one of the earliest developments being the theory of sequential sampling in statistics, as developed by 
This dealt mostly with making sequential decisions for accepting one among a set of particular hypothesis, with a view towards applying it to jointly decide the termination of an experiment and the acceptance of a hypothesis
A more general overview of sequential decision problems from a Bayesian viewpoint is offered in 
The optimal, but intractable, Bayesian solution for bandit problems was given in , while recently tight bounds on the sample complexity of exploration have been found 
An approximation to the full Bayesian case for the general reinforcement learning problem is given in , while an alternative technique based on eliminating actions which are confidently estimated as low-value is given in 
The following section formulates the intuitive concept of trading exploration and exploitation as a natural consequence of the definition of the problem of reinforcement learning
After the problem definitions which correspond to either extreme are identified, Sec
derives a threshold for switching from exploratory to greedy behaviour in bandit problems
This threshold is found to depend on the effective reward horizon of the optimal policy and on our current belief distribution of the expected rewards of each action
A sketch of the extension to MDPs is presented in Section uses an upper bound on the value of exploration to derive practical algorithms, which are then illustrated experimentally in Sec
We conclude with a discussion on the relations with other methods
We present a novel approach to semi-supervised learning which is based on statistical physics
Most of the former work in the field of semi-supervised learning classifies the points by minimizing a certain energy function, which corresponds to a minimal k-way cut solution
In contrast to these methods, we estimate the distribution of classifications, instead of the sole minimal k-way cut, which yields more accurate and robust results
Our approach may be applied to all energy functions used for semi-supervised learning
The method is based on sampling using a Multicanonical Markov chain Monte-Carlo algorithm, and has a straightforward probabilistic interpretation, which allows for soft assignments of points to classes, and also to cope with yet unseen class types
The suggested approach is demonstrated on a toy data set and on two real-life data sets of gene expression
Situations in which many unlabelled points are available and only few labelled points are provided call for semi-supervised learning methods
The goal of semi-supervised learning is to classify the unlabelled points, on the basis of their distribution and the provided labelled points
Such problems occur in many fields, in which obtaining data is cheap but labelling is expensive
In such scenarios supervised methods are impractical, but the presence of the few labelled points can significantly improve the performance of unsupervised methods
The basic assumption of unsupervised learning, i e clustering, is that points which belong to the same cluster actually originate from the same class
Clustering methods which are based on estimating the density of data points define a cluster as a `mode' in the distribution, i e a relatively dense region surrounded by relatively lower density
Hence each mode is assumed to originate from a single class, although a certain class may be dispersed over several modes
In case the modes are well separated they can be easily identified by unsupervised techniques, and there is no need for semi-supervised methods
However, consider the case of two close modes which belong to two different classes, but the density of points between them is not significantly lower than the density within each mode
In this case density based unsupervised methods may encounter difficulties in distinguishing between the modes classes , while semi-supervised methods can be of help
Even if a few points are labelled in each class, semi-supervised algorithms, which cannot cluster together points of different labels, are forced to place a border between the modes
Most probably the border will pass in between the modes, where the density of points is lower
Hence, the forced border `amplifies' the otherwise less noticed differences between the modes
For example, consider the image here
Each pixel corresponds to a data point and the similarity score between adjacent pixels is of value unity
The green and red pixels are labelled while the rest of the blue pixels are unlabelled
The desired classification into red and green classes appears in Fig b
It is unlikely that any unsupervised method would partition the data correctly see eg Fig c since the two classes form one uniform cluster
However, using a few labelled points semi-supervised methods which must place a border between the red and green classes may become useful
In recent years various types of semi-supervised learning algorithms have been proposed, however almost all of these methods share a common basic approach
They define a certain cost function, i e energy , over the possible classifications, try to minimize this energy, and output the minimal energy classification as their solution
Different methods vary by the specific energy function and by their minimization procedures; for example the work on graph cuts , minimizes the cost of a cut in the graph, while others choose to minimize the normalized cut cost , or a quadratic cost 
As stated recently by , searching for a minimal energy has a basic disadvantage, common to all former methods: it ignores the robustness of the found solution
Blum et al mention the case of several minima with equal energy, where one arbitrarily chooses one solution, instead of considering them all
Put differently, imagine the energy landscape in the space of solutions; it may contain many equal energy minima as considered Blum et al , but also other phenomena may harm the robustness of the global minimum as an optimal solution
First, it may happen that the difference in energy between the global minimum, and close by solutions is minuscule, thus picking the minimum as the sole solution may be incorrect or arbitrary
Secondly, in many cases there are too few data points both labelled and unlabelled which may cause the empirical density to locally deviate from the true density
Such fluctuations in the density may drive the minimal energy solution far from the correct one
For example, due to fluctuations a low density ``crack" may be formed inside a high density region, which may erroneously split a single cluster in two
Another type of fluctuation may generate a ``filament" of high density points in a low density region, which may unite two clusters of different classes
In both cases, the minimal energy solution is erroneously `guided' by the fluctuations, and fails to find the correct classification
An example of the latter case appears in Fig a; the classifications provided by three semi-supervised methods appear in Fig d f, fail to recover the desired classification, due to a `filament' which connects the classes *    Searching for the minimal energy solution is equivalent to seeking the most probable joint classification MAP
A possible remedy to the difficulties in this approach may then be to consider the probability distribution of all possible classifications
Blum et al provided a first step in this direction using a randomized min-cut algorithm
In this work we provide a different solution based on statistical physics
Basically each solution in our method is weighed by its energy SYMBOL , also known as the Boltzmann weight, and its probability is given by: SYMBOL     where the ``temperature'' SYMBOL serves as a free parameter, and the energy SYMBOL takes into account both unlabelled and labelled points
Classification is then performed by marginalizing , thus estimating the probability that a point SYMBOL belongs to a class SYMBOL
This formalism is often referred to as a Markov random field MRF , which has been applied in numerous works, including in the context of semi-supervised learning by 
However, they seek the MAP solution which corresponds to SYMBOL , while we estimate the distribution itself at SYMBOL 
Using the framework of statistical physics has several advantages in the context of semi-supervised learning: First, classification has a simple probabilistic interpretation
It yields a fuzzy assignment of points to class types, which may also serve as a confidence level in the classification
Secondly, since exactly estimating the marginal probabilities is, in most cases, intractable, statistical physics has developed elegant Markov chain Monte-Carlo MCMC methods which are suitable for estimating semi-supervised systems
Due to the inherent complexity of semi-supervised problems, `standard' MCMC methods, such as the Metropolis and Swendsen-Wang methods provide poor results, and one needs to apply more sophisticated algorithms, as discussed in section 
Thirdly, using statistical physics allows us to gain an intuition regarding the nature of a semi-supervised problem, i e , it allows for a detailed analysis of the effect of adding labelled points to an unlabelled data set
In addition, our method also has two practical advantages: i while most semi-supervised learning methods consider only the case of two class types, our method is naturally extended to the multi-class scenario ii Another unique feature of our method is its ability to suggest the existence of a new class type, which did not appear in the labelled set
Our main objective in this paper is to present a framework, which can later be applied in different directions
For example, the energy function in can be any of the functions used in other semi-supervised methods
In this paper we chose to use the min-cut cost function
We do not claim that using this cost function is optimal, and indeed we observed that it is suboptimal in some cases
However, we aim to convince the reader that applying our method, to any energy function, would always yield equal or better results than merely minimizing the same energy function
Our work is closely related to the typical cut criterion for unsupervised learning, first introduced by in the framework of statistical physics and later in a graph theoretic context by 
The method introduced in this work can be viewed as an extension of these clustering algorithms to the semi-supervised case
The paper is organized as follows: Section presents the model, and Section discusses the issue of estimating marginal probabilities
Section presents the qualitative effect of adding labelled points
Our semi-supervised algorithm is outlined in Section 
Section demonstrates the performance of our algorithm on a toy data set and on two real-life examples of gene expression data
The adaptation rule for Vector Quantization algorithms, and consequently the convergence of the generated sequence, depends on the existence and properties of a function called the energy function, defined on a topological manifold
Our aim is to investigate the conditions of existence of such a function for a class of algorithms examplified by the initial "K-means" and Kohonen algorithms 
The results presented here supplement previous studies, including , , , and 
Our work shows that the energy function is not always a potential but at least the uniform limit of a series of potential functions which we call a pseudo-potential
It also shows that a large number of existing vector quantization algorithms developed by the Artificial Neural Networks community fall into this category
The framework we define opens the way to study the convergence of all the corresponding adaptation rules at once, and a theorem gives promising insights in that direction
We also demonstrate that the "K-means" energy function is a pseudo-potential but not a potential in general
Consequently, the energy function associated to the "Neural-Gas" is not a potential in general
In vector quantization theory , a set of prototypes SYMBOL is placed on a manifold SYMBOL , in order to minimize the following integral function, called the "energy function": SYMBOL     where SYMBOL indicates the probability density defined on SYMBOL
We focus on the stochastic iterative approaches where at each time step, a datum SYMBOL is drawn from the probability density function pdf SYMBOL , and the prototypes SYMBOL are adapted according to SYMBOL using the adaptation rule: SYMBOL     where the adaptation step is tuned using the parameter SYMBOL generally decreasing over the time SYMBOL is taken thereafter equal to 1 without restricting the general results , and SYMBOL is a "neighborhood" function particular to each vector quantization algorithm
Here we focus on discontinuous SYMBOL functions
A main concern in the field of Vector Quantization, is to decide whether the adaptation rule corresponds or not to a stochastic gradient descent along the energy function , i e whether this energy function is or is not a potential onto the entire manifold SYMBOL
On one hand, if the energy function is a potential then the convergence of the prototypes obeying their adaptation rule toward a minimum of this energy function is well established, in particular in the stochastic optimization framework with which this paper is concerned
For example, the energy function associated to the K-means algorithm , stochastic version of the LBG algorithm of Linde et al , is a potential as long as the pdf SYMBOL is continuous 
On the other hand, if the energy function is not a potential, then very few is known about the convergence of the corresponding adaptation rule
For example, several results have already shown that for a continuous density SYMBOL , the corresponding vector adaptation rule of the Kohonen Self-Organizing Map SOM algorithm  does not correspond to a stochastic gradient descent along a global energy function, and the convergence, although being observed in practice, turns out to be very difficult to prove, not to mention that most of the efforts have been carried out on the Kohonen rule 
All the vector quantization algorithms we study in this paper are variants of the K-means algorithm as we will see in section
We know these algorithms converge in practice toward acceptable value of their energy functions whenever they are proved to be associated or not to potentials
However, the theoretical study of their convergence is not available, so they remain largely heuristics
Among all these algorithms, the Neural-Gas deserves a particular attention
It has been claimed by its authors to be associated to a global potential in general, hence to a converging adaptation rule
We propose a counter-example with a discontinuous pdf SYMBOL which demonstrates that this claim is not true
This shows that the study of the convergence of all these algorithms is still in its infancy and motivates the present work
In this paper, we propose a framework which encompasses all these algorithms
We study this framework and we demonstrate that the energy function associated to these algorithms is not a potential in general
We also demonstrate that this energy function belongs to a broad class of functions which includes potential functions as a special case
The energy functions within this class are called "pseudo-potentials"
The results we obtain do not depend on the continuity of the probability density function SYMBOL , and give a first step toward an explanation why all the algorithms shown to belong to this framework succeed, in practice, in minimizing their associated energy function whether they are potentials or not
This framework should open up further avenues for a general study of the convergence properties of all the algorithms it contains at once
In section 2, we present the framework of this study
In section 3, we define a "pseudo-potential" function, which can be approximated by a series of potential functions: we define the concept of cellular manifold and this series of potentials
In section 4, we give the main theorem which states that an energy function of that framework is necessarily a pseudo-potential
We consider the K-means to show that pseudo-potentials are not always potentials
We discuss the consequence on the convergence of the corresponding adaptation rule
In section 5, we show that most of the common vector quantization algorithms belong to that framework
We consider semi-supervised classification when part of the available data is unlabeled
These unlabeled data can be useful for the classification problem when we make an assumption relating the behavior of the regression function to that of the marginal distribution
Seeger proposed the well-known cluster assumption as a reasonable one
We propose a mathematical formulation of this assumption and a method based on density level sets estimation that takes advantage of it to achieve fast rates of convergence both in the number of unlabeled examples and the number of labeled examples
Semi-supervised classification has been of growing interest over the past few years and many methods have been proposed
The methods try to give an answer to the question: How to improve classification accuracy using unlabeled data together with the labeled data
Unlabeled data can be used in different ways depending on the assumptions on the model
There are two types of assumptions
The first one consists in assuming that we have a set of potential classifiers and we want to aggregate them
In that case, unlabeled data is used to measure the compatibility between the classifiers and reduces the complexity of the resulting classifier see, eg , , 
The second approach is the one that we use here
It assumes that the data contains clusters that have homogeneous labels and the unlabeled observations are used to identify these clusters
This is the so-called cluster assumption
This idea can be put in practice in several ways giving rise to various methods
The simplest is the one presented here: estimate the clusters, then label each cluster uniformly
Most of these methods use Hartigan's definition of clusters, namely the connected components of the density level sets
However, they use a parametric usually mixture model to estimate the underlying density which can be far from reality
Moreover, no generalization error bounds are available for such methods
In the same spirit, and propose methods that learn a distance using unlabeled data in order to have intra-cluster distances smaller than inter-clusters distances
The whole family of graph-based methods aims also at using unlabeled data to learn the distances between points
The edges of the graphs reflect the proximity between points
For a detailed survey on graph methods we refer to 
Finally, we mention kernel methods, where unlabeled data are used to build the kernel
Recalling that the kernel measures proximity between points, such methods can also be viewed as learning a distance using unlabeled data see , , 
The cluster assumption can be interpreted in another way, i e , as the requirement that the decision boundary has to lie in low density regions
This interpretation has been widely used in learning since it can be used in the design of standard algorithms such as Boosting , or SVM , , which are closely related to kernel methods mentioned above
In these algorithms, a greater penalization is given to decision boundaries that cross a cluster
For more details, see, eg , , , 
Although most methods make, sometimes implicitly, the cluster assumption, no formulation in probabilistic terms has been provided so far
The formulation that we propose in this paper remains very close to its original text formulation and allows to derive generalization error bounds
We also discuss what can and cannot be done using unlabeled data
One of the conclusions is that considering the whole excess-risk is too ambitious and we need to concentrate on a smaller part of it to observe the improvement of semi-supervised classification over standard classification
Outline of the paper
After describing the model, we formulate the cluster assumption and discuss why and how it can improve classification performance in the next section
In Section , we study the population case when the marginal density SYMBOL is known, to get an idea of our target
Indeed, such a population case corresponds in some way to the case when the amount of unlabeled data is infinite
Section contains the main result: we propose an algorithm for which we derive rates of convergence for the SYMBOL -thresholded excess-risk as a measure of performance
An exemple of consistent density level set estimators is given in Section 
Section is devoted to discussion on the choice of SYMBOL and possible improvements
Proofs of the results are gathered in Section 3
Throughout the paper, we denote by SYMBOL positive constants
We write SYMBOL for the complement of the set SYMBOL
For two sequences SYMBOL and SYMBOL in that paper, SYMBOL will be SYMBOL or SYMBOL , we write SYMBOL if there exists a constant SYMBOL such that SYMBOL and we write SYMBOL if SYMBOL for some constants SYMBOL
Thus, if SYMBOL , we have SYMBOL , for any SYMBOL
Solomonoff completed the Bayesian framework by providing a rigorous, unique, formal, and universal choice for the model class and the prior
We discuss in breadth how and in which sense universal non- iid     sequence prediction solves various philosophical problems of traditional Bayesian sequence prediction
We show that Solomonoff's model possesses many desirable properties: Fast convergence and strong bounds, and in contrast to most classical continuous prior densities has no zero p oste rior problem, ie     can confirm universal hypotheses, is reparametrization and regrouping invariant, and avoids the old-evidence and updating problem
It even performs well actually better in non-computable environments
Given the weather in the past, what is the probability of rain tomorrow
What is the correct answer in an IQ test asking to continue the sequence 1,4,9,16,
Given historic stock-charts, can one predict the quotes of tomorrow
Assuming the sun rose 5000 years every day, how likely is doomsday that the sun does not rise tomorrow
These are instances of the important problem of inductive inference or time-series forecasting or sequence prediction
Finding prediction rules for every particular new problem is possible but cumbersome and prone to disagreement or contradiction
What we are interested in is a formal general theory for prediction
The Bayesian framework is the most consistent and successful framework developed thus far 
A Bayesian considers a set of environments=   -hypotheses=   -models SYMBOL which includes the true data generating probability distribution SYMBOL
From one's prior belief SYMBOL in environment SYMBOL and the observed data sequence SYMBOL , Bayes' rule yields one's posterior confidence in SYMBOL
In a predictive setting, one directly determines the predictive probability of the next symbol SYMBOL without the intermediate step of identifying a true or good or causal or useful model
Note that classification and regression can be regarded as special sequence prediction problems, where the sequence SYMBOL of SYMBOL -pairs is given and the class label or function value SYMBOL shall be predicted
The Bayesian framework leaves open how to choose the model class SYMBOL and prior SYMBOL
General guidelines are that SYMBOL should be small but large enough to contain the true environment SYMBOL , and SYMBOL should reflect one's prior subjective belief in SYMBOL or should be non-informative or neutral or objective if no prior knowledge is available
But these are informal and ambiguous considerations outside the formal Bayesian framework
Solomonoff's rigorous, essentially unique, formal, and universal solution to this problem is to consider a single large universal class SYMBOL suitable for all induction problems
The corresponding universal prior SYMBOL is biased towards simple environments in such a way that it dominates=   -superior to all other priors
This leads to an a priori probability SYMBOL which is equivalent to the probability that a universal Turing machine with random input tape outputs SYMBOL
Many interesting, important, and deep results have been proven for Solomonoff's universal distribution SYMBOL 
The motivation and goal of this paper is to provide a broad discussion of how and in which sense universal sequence prediction solves all kinds of philosophical problems of Bayesian sequence prediction, and to present some recent results
Many arguments and ideas could be further developed
I hope that the exposition stimulates such a future, more detailed, investigation
In Section we review the excellent predictive performance of Bayesian sequence prediction for generic non- iid     countable and continuous model classes
Section critically reviews the classical principles indifference, symmetry, minimax for obtaining objective priors, introduces the universal prior inspired by Occam's razor and quantified in terms of Kolmogorov complexity
In Section for iid     SYMBOL and Section for universal SYMBOL we show various desirable properties of the universal prior and class non-zero p oste rior, confirmation of universal hypotheses, reparametrization and regrouping invariance, no old-evidence and updating problem in contrast to most classical continuous prior densities
Finally, we show that the universal mixture performs better than classical continuous mixtures, even in uncomputable environments
Section 2 contains critique and summary
A fundamental problem in artificial intelligence is that nobody really knows what intelligence is
The problem is especially acute when we need to consider artificial systems which are significantly different to humans
In this paper we approach this problem in the following way: We take a number of well known informal definitions of human intelligence that have been given by experts, and extract their essential features
These are then mathematically formalised to produce a general measure of intelligence for arbitrary machines
We believe that this measure formally captures the concept of machine intelligence in the broadest reasonable sense
Most of us think that we recognise intelligence when we see it, but we are not really sure how to precisely define or measure it
We informally judge the intelligence of others by relying on our past experiences in dealing with people
Naturally, this naive approach is highly subjective and imprecise
A more principled approach would be to use one of the many standard intelligence tests that are available
Contrary to popular wisdom, these tests, when correctly applied by a professional, deliver statistically consistent results and have considerable power to predict the future performance of individuals in many mentally demanding tasks
However, while these tests work well for humans, if we wish to measure the intelligence of other things, perhaps of a monkey or a new machine learning algorithm, they are clearly inappropriate
One response to this problem might be to develop specific kinds of tests for specific kinds of entities; just as intelligence tests for children differ to intelligence tests for adults
While this works well when testing humans of different ages, it comes undone when we need to measure the intelligence of entities which are profoundly different to each other in terms of their cognitive capacities, speed, senses, environments in which they operate, and so on
To measure the intelligence of such diverse systems in a meaningful way we must step back from the specifics of particular systems and establish the underlying fundamentals of what it is that we are really trying to measure
That is, we need to establish a notion of intelligence that goes beyond the specifics of particular kinds of systems
The difficulty of doing this is readily apparent
Consider, for example, the memory and numerical computation tasks that appear in some intelligence tests and which were once regarded as defining hallmarks of human intelligence
We now know that these tasks are absolutely trivial for a machine and thus do not test the machine's intelligence
Indeed even the mentally demanding task of playing chess has been largely reduced to brute force search
As technology advances, our concept of what intelligence is continues to evolve with it
How then are we to develop a concept of intelligence that is applicable to all kinds of systems
Any proposed definition must encompass the essence of human intelligence, as well as other possibilities, in a consistent way
It should not be limited to any particular set of senses, environments or goals, nor should it be limited to any specific kind of hardware, such as silicon or biological neurons
It should be based on principles which are sufficiently fundamental so as to be unlikely to alter over time
Furthermore, the intelligence measure should ideally be formally expressed, objective, and practically realisable
This paper approaches this problem in the following way
In Section we consider a range of definitions of human intelligence that have been put forward by well known psychologists
From these we extract the most common and essential features and use them to create an informal definition of intelligence
Section then introduces the framework which we use to construct our formal measure of intelligence
This framework is formally defined in Section
In Section we use our developed formalism to produce a formal definition of intelligence
Section closes with a short summary
A preliminary sketch of the ideas in this paper appeared in the poster 
It can be shown that the intelligence measure presented here is in fact a variant of the Intelligence Order Relation that appears in the theory of AIXI, the provably optimal universal agent 
A long journal version of this paper is being written in which we give the proposed measure of machine intelligence and its relation to other such tests a much more comprehensive treatment
Naturally, we expect such a bold initiative to be met with resistance
However, we hope that the reader will appreciate the value of our approach: With a formally precise definition put forward we aim to better our understanding of what is a notoriously subjective and slippery concept
Consider an agent interacting with an environment in cycles
In every interaction cycle the agent is rewarded for its performance
We compare the average reward SYMBOL from cycle SYMBOL to SYMBOL average value with the future discounted reward SYMBOL from cycle SYMBOL to SYMBOL discounted value
We consider essentially arbitrary non-geometric discount sequences and arbitrary reward sequences non-MDP environments
We show that asymptotically SYMBOL for SYMBOL and SYMBOL for SYMBOL are equal, provided both limits exist
Further, if the effective horizon grows linearly with SYMBOL or faster, then the existence of the limit of SYMBOL implies that the limit of SYMBOL exists
Conversely, if the effective horizon grows linearly with SYMBOL or slower, then existence of the limit of SYMBOL implies that the limit of SYMBOL exists
We consider the reinforcement learning setup , where an agent interacts with an environment in cycles
In cycle SYMBOL , the agent outputs acts SYMBOL , then it makes observation SYMBOL and receives reward SYMBOL , both provided by the environment
Then the next cycle SYMBOL starts
For simplicity we assume that agent and environment are deterministic
Typically one is interested in action sequences, called plans or policies, for agents that result in high reward
The simplest reasonable measure of performance is the total reward sum or equivalently the average reward, called average value SYMBOL , where SYMBOL should be the lifespan of the agent
One problem is that the lifetime is often not known in advance, eg     often the time one is willing to let a system run depends on its displayed performance
More serious is that the measure is indifferent to whether an agent receives high rewards early or late if the values are the same
A natural non-arbitrary choice for SYMBOL is to consider the limit SYMBOL
While the indifference may be acceptable for finite SYMBOL , it can be catastrophic for SYMBOL
Consider an agent that receives no reward until its first action is SYMBOL , and then once receives reward SYMBOL
For finite SYMBOL , the optimal SYMBOL to switch from action SYMBOL to SYMBOL is SYMBOL
Hence SYMBOL for SYMBOL , so the reward maximizing agent for SYMBOL actually always acts with SYMBOL , and hence has zero reward, although a value arbitrarily close to 1 would be achievable Immortal agents are lazy 
More serious, in general the limit SYMBOL may not even exist
Another approach is to consider a moving horizon
In cycle SYMBOL , the agent tries to maximize SYMBOL , where SYMBOL increases with SYMBOL , eg     SYMBOL with SYMBOL being the horizon
This naive truncation is often used in games like chess plus a heuristic reward in cycle SYMBOL to get a reasonably small search tree
While this can work in practice, it can lead to inconsistent optimal strategies, i e     to agents that change their mind
Consider the example above with SYMBOL
In every cycle SYMBOL it is better first to act SYMBOL and then SYMBOL SYMBOL , rather than immediately SYMBOL SYMBOL , or SYMBOL SYMBOL 
But entering the next cycle SYMBOL , the agent throws its original plan overboard, to now choose SYMBOL in favor of SYMBOL , followed by SYMBOL
This pattern repeats, resulting in no reward at all
The standard solution to the above problems is to consider geometrically=exponentially discounted reward 
One discounts the reward for every cycle of delay by a factor SYMBOL , i e     considers SYMBOL
The SYMBOL maximizing policy is consistent in the sense that its actions SYMBOL coincide with the optimal policy based on SYMBOL
At first glance, there seems to be no arbitrary lifetime SYMBOL or horizon SYMBOL , but this is an illusion
SYMBOL is dominated by contributions from rewards SYMBOL , so has an effective horizon SYMBOL
While such a sliding effective horizon does not cause inconsistent policies, it can nevertheless lead to suboptimal behavior
For every effective horizon, there is a task that needs a larger horizon to be solved
For instance, while SYMBOL is sufficient for tic-tac-toe, it is definitely insufficient for chess
There are elegant closed form solutions for Bandit problems, which show that for any SYMBOL , the Bayes-optimal policy can get stuck with a suboptimal arm is not self-optimizing 
For SYMBOL , SYMBOL , and the defect decreases
There are various deep papers considering the limit SYMBOL , and comparing it to the limit SYMBOL 
The analysis is typically restricted to ergodic MDPs for which the limits SYMBOL and SYMBOL exist
But like the limit policy for SYMBOL , the limit policy for SYMBOL can display very poor performance, i e     we need to choose SYMBOL fixed in advance but how , or consider higher order terms
We also cannot consistently adapt SYMBOL with SYMBOL
Finally, the value limits may not exist beyond ergodic MDPs
There is little work on other than geometric discounts
In the psychology and economics literature it has been argued that people discount a one day=cycle delay in reward more if it concerns rewards now rather than later, eg     in a year plus one day
So there is some work on ``sliding'' discount sequences SYMBOL
One can show that this also leads to inconsistent policies if SYMBOL is non-geometric 
Is there any non-geometric discount leading to consistent policies
In the generally discounted value SYMBOL with SYMBOL has been introduced
It is well-defined for arbitrary environments, leads to consistent policies, and eg     for quadratic discount SYMBOL to an increasing effective horizon proportionally to SYMBOL , i e     the optimal agent becomes increasingly farsighted in a consistent way, leads to self-optimizing policies in ergodic SYMBOL th-order MDPs in general, Bandits in particular, and even beyond MDPs
See for these and for more results
The only other serious analysis of general discounts we are aware of is in , but their analysis is limited to Bandits and so-called regular discount
This discount has bounded effective horizon, so also does not lead to self-optimizing policies
The asymptotic total average performance SYMBOL and future discounted performance SYMBOL are of key interest
For instance, often we do not know the exact environment in advance but have to learn it from past experience, which is the domain of reinforcement learning and adaptive control theory 
Ideally we would like a learning agent that performs asymptotically as well as the optimal agent that knows the environment in advance
The subject of study of this paper is the relation between SYMBOL and SYMBOL for general discount SYMBOL and arbitrary environment
The importance of the performance measures SYMBOL and SYMBOL , and general discount SYMBOL has been discussed above
There is also a clear need to study general environments beyond ergodic MDPs, since the real world is neither ergodic e g     losing an arm is irreversible nor completely observable
The only restriction we impose on the discount sequence SYMBOL is summability SYMBOL so that SYMBOL exists, and monotonicity SYMBOL 
Our main result is that if both limits SYMBOL and SYMBOL exist, then they are necessarily equal Section , Theorem 
Somewhat surprisingly this holds for any discount sequence SYMBOL and any environment reward sequence SYMBOL , whatsoever
Note that limit SYMBOL may exist or not, independent of whether SYMBOL exists or not
We present examples of the four possibilities in Section
Under certain conditions on SYMBOL , existence of SYMBOL implies existence of SYMBOL , or vice versa
We show that if a quantity closely related to the effective horizon grows linearly with SYMBOL or faster, then existence of SYMBOL implies existence of SYMBOL and their equality Section , Theorem 
Conversely, if the effective horizon grows linearly with SYMBOL or slower, then existence of SYMBOL implies existence of SYMBOL and their equality Section , Theorem 
Note that apart from discounts with oscillating effective horizons, this implies and this is actually the path used to prove the first mentioned main result
In Sections and we define and provide some basic properties of average and discounted value, respectively
In a recent breakthrough,    Bshouty et al , 2005   obtained the first passive-lear   -ning algorithm for DNFs under the uniform distribution
They showed that DNFs are learnable in the Random Walk and Noise Sensitivity models
We extend their results in several directions
We first show that thresholds of parities, a natural class encompassing DNFs, cannot be learned efficiently in the Noise Sensitivity model using only statistical queries
In contrast, we show that a cyclic version of the Random Walk model allows to learn efficiently polynomially weighted thresholds of parities
We also extend the algorithm of Bshouty et al to the case of Unions of Rectangles, a natural generalization of DNFs to SYMBOL